{"cells":[{"cell_type":"markdown","source":["**Neural Network for duck bones Image Classification based on tipology**\n","\n","**Author**: Elisa Paperini, Nevio Dubbini\n","\n","**License**: CC-BY-SA 4.0\n","\n","**Year**: 2024 (last version)\n","\n","**Description**\n","\n","This script, executed on Google Colab, uses PyTorch to fine-tune a pre-trained VGG16 model with IMAGENET1K_V1 weights for a 5-class classification task. A custom fully connected layer replaces the original classifier's last layer.\n","\n","The model's performance is evaluated using a training-validation-test set split, balancing computational cost and accuracy. Hyperparameters are optimized via grid search. The Adam optimizer and CrossEntropyLoss functions are employed. The network is trained for 100 epochs.\n"],"metadata":{"id":"ZQ6SOWizv2pH"},"id":"ZQ6SOWizv2pH"},{"cell_type":"markdown","id":"GB1o_9loqZUv","metadata":{"id":"GB1o_9loqZUv"},"source":["### Set the environment"]},{"cell_type":"code","execution_count":null,"id":"ab31e4f3-1e76-4a2d-bf67-5866e8e30ad0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ab31e4f3-1e76-4a2d-bf67-5866e8e30ad0","outputId":"892829a8-89a8-4c90-8695-f04f2d5f9653"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyunpack\n","  Downloading pyunpack-0.3-py2.py3-none-any.whl.metadata (863 bytes)\n","Collecting easyprocess (from pyunpack)\n","  Downloading EasyProcess-1.1-py3-none-any.whl.metadata (855 bytes)\n","Collecting entrypoint2 (from pyunpack)\n","  Downloading entrypoint2-1.1-py2.py3-none-any.whl.metadata (1.0 kB)\n","Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n","Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n","Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n","Installing collected packages: entrypoint2, easyprocess, pyunpack\n","Successfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\n"]}],"source":["# Load libraries\n","\n","# OS and file handling libraries\n","import os\n","import glob\n","import pathlib\n","from PIL import Image\n","import regex as re\n","\n","# Numerical computing and data manipulation\n","import numpy as np\n","import itertools # Useful for iterating over multiple variables\n","import matplotlib.pyplot as plt # Plotting utilities\n","\n","# PyTorch for deep learning\n","import torch\n","import torch.nn as nn # Neural network modules\n","import torch.nn.functional as F # Functional API for layers and activation functions\n","from torch.utils.data import Dataset, DataLoader, Subset # Data loading utilities\n","from torch.optim import Adam, lr_scheduler # Optimizer and learning rate scheduler\n","\n","# Torchvision for image processing and pre-trained models\n","import torchvision\n","from torchvision import datasets, models, transforms # Datasets, pre-trained models, and data transformations\n","import torchvision.utils # Utility functions for visualization\n","import torchvision.datasets as dsets # Alternative dataset module\n","from torchvision.transforms import v2  # Advanced image transformations (newer API in torchvision)\n","\n","# Computer vision utilities (OpenCV)\n","import cv2 # Used for image processing\n","\n","# Scikit-learn for model evaluation metrics and k-fold cross-validation\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","from sklearn.model_selection import KFold\n","\n","# Google Colab-specific library for mounting Google Drive (needed for loading data from Drive)\n","from google.colab import drive # Only required when using Google Colab\n","\n","# Library for extracting .zip archives\n","!pip install pyunpack  # Install pyunpack if not already installed\n","from pyunpack import Archive  # Used for extracting compressed files"]},{"cell_type":"code","execution_count":null,"id":"cbfa4b40-5af0-438b-a51e-4047985c458c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbfa4b40-5af0-438b-a51e-4047985c458c","outputId":"bc0da795-f8f0-4daf-ed49-5679d63d8ba8"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["# Checking for availability of a CUDA-enabled GPU and assigns the device accordingly.\n","# If a GPU is available, it will be used for computations; otherwise, the CPU will be used.\n","device = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.cuda.empty_cache()\n","print(device)"]},{"cell_type":"code","execution_count":null,"id":"mgpHp5BkfzSf","metadata":{"id":"mgpHp5BkfzSf"},"outputs":[],"source":["# Import the 'drive' module for interacting with Google Drive\n","from google.colab import drive\n","# Mount Google Drive to the '/content/drive' directory\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"d14fa6ff-0c07-4799-a1ab-beda71b3959a","metadata":{"id":"d14fa6ff-0c07-4799-a1ab-beda71b3959a"},"outputs":[],"source":["# Unzip the zip folder (replace with your path) to the directory where images are stored ('/tmp').\n","# The directory is deleted when the session is ended.\n","Archive('/content/drive/MyDrive/bones_detection_tipology.zip').extractall('/tmp')"]},{"cell_type":"markdown","id":"zIFK6gAOLmYH","metadata":{"id":"zIFK6gAOLmYH"},"source":["### Data preprocessing"]},{"cell_type":"code","execution_count":null,"id":"ca44fd76-5c76-4034-892d-5a4cd3e29453","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca44fd76-5c76-4034-892d-5a4cd3e29453","outputId":"ff815d02-7437-4429-ddba-403fe659f40d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n","  warnings.warn(\n"]}],"source":["# Preprocessing steps applied to training data\n","train_transform = v2.Compose([\n","                  v2.Resize(size=(224, 224), antialias=True), # Resize images, using antialiasing\n","                  v2.RandomRotation(degrees=(-2, 2)), # Randomly rotate images in the [-2. 2] degrees interval\n","                  v2.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 0.5)), # Apply a Gaussian blur to the images\n","                  v2.ToTensor(),  # Change the pixel range from 0-255 to 0-1, numpy to tensors\n","                  v2.ToDtype(torch.float32, scale=True), # Convert the tensor to the torch.float32 data type required for PyTorch\n","                  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize the image tensor using the specified mean and standard deviation\n","                  ])\n","\n","# Preprocessing steps applied to validation data\n","val_transform = v2.Compose([\n","                # v2.ToImage(),  # Convert to tensor (only needed if you had a PIL image) : [0, 255] -> [0, 1]\n","                v2.Resize(size=(224, 224), antialias=True),  # Resize images, using antialiasing\n","                v2.RandomRotation(degrees=(-2, 2)), # Randomly rotate images in the [-2. 2] degrees interval\n","                v2.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 0.5)), # Apply a Gaussian blur to the images\n","                v2.ToTensor(),  # Change the pixel range from 0-255 to 0-1, numpy to tensors\n","                v2.ToDtype(torch.float32, scale=True),    # Convert the tensor to the torch.float32 data type required for PyTorch\n","                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize the image tensor using the specified mean and standard deviation\n","                ])"]},{"cell_type":"code","execution_count":null,"id":"a5036f87-1207-4bf9-9b36-018709d091a2","metadata":{"id":"a5036f87-1207-4bf9-9b36-018709d091a2"},"outputs":[],"source":["# Path to training and validation directory (replace with your paths)\n","data_dir = '/tmp' # Temporary directory where the data was extracted\n","train_path = os.path.join(data_dir, 'bones_train') # Path to the training directory\n","val_path =  os.path.join(data_dir, 'bones_validation') # Path to the validation directory\n","\n","# Load original images and labels, apply transforms for training and validation\n","train_data = dsets.ImageFolder(root=train_path, transform=train_transform)\n","val_data = dsets.ImageFolder(root=val_path, transform=val_transform)\n","\n","# Loading dataset for training and validation\n","batch_size_real = 8 # Batch size\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_real, shuffle=True, num_workers=2, persistent_workers=True) # Create the training data loader\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size_real, shuffle=True, num_workers=2, persistent_workers=True) # Create the validation data loader\n","\n","class_names_tr = train_data.classes # Extract the class names from the training dataset\n","class_names_va = val_data.classes # Extract the class names from the validation dataset"]},{"cell_type":"code","execution_count":null,"id":"xtHon-u90mIC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtHon-u90mIC","outputId":"4dca5420-611c-4dec-a211-461a8608400b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation\n","Dataset ImageFolder\n","    Number of datapoints: 367\n","    Root location: /tmp/bones_validation\n","    StandardTransform\n","Transform: Compose(\n","                 ToImage()\n","                 Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n","                 RandomRotation(degrees=[-2.0, 2.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n","                 GaussianBlur(kernel_size=(5, 5), sigma=[0.1, 0.5])\n","                 ToDtype(scale=True)\n","                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n","           )\n","------\n","Training\n","Dataset ImageFolder\n","    Number of datapoints: 1283\n","    Root location: /tmp/bones_train\n","    StandardTransform\n","Transform: Compose(\n","                 Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n","                 RandomRotation(degrees=[-2.0, 2.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n","                 GaussianBlur(kernel_size=(5, 5), sigma=[0.1, 0.5])\n","                 ToTensor()\n","                 ToDtype(scale=True)\n","                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n","           )\n"]}],"source":["# Print information about the validation dataset\n","print('Validation Dataset:')\n","print(f'Total samples: {len(val_loader.dataset)}')  # Display the number of samples in the validation set\n","print(f'Dataset type: {type(val_loader.dataset)}')  # Show the dataset type\n","print('------')\n","\n","# Print information about the training dataset\n","print('Training Dataset:')\n","print(f'Total samples: {len(train_loader.dataset)}')  # Display the number of samples in the training set\n","print(f'Dataset type: {type(train_loader.dataset)}')  # Show the dataset type"]},{"cell_type":"code","execution_count":null,"id":"44260ead-af46-4b1d-a991-0f686514278d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44260ead-af46-4b1d-a991-0f686514278d","outputId":"0b85a994-f547-4f19-c114-eb77214aef1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1283 367\n","['CMC', 'COR', 'HUM', 'TMT', 'TT']\n","5\n"]}],"source":["# Calculate the number of training and validation images\n","train_count=len(glob.glob(train_path+'/**/*.jpg')) # Count all JPG files recursively in the training directory\n","val_count=len(glob.glob(val_path+'/**/*.jpg')) # Count all JPG files recursively in the validation directory\n","\n","print('Number of images in train dataset:', train_count)\n","print('Number of images in validation dataset:', val_count)\n","\n","# Retrieve the class categories (labels)\n","root=pathlib.Path(train_path) # Convert training path to a pathlib object for easier file handling\n","classes=sorted([j.name.split('/')[-1] for j in root.iterdir()]) # Extract class names from folder names and sort them\n","\n","print('Class labels:', classes)\n","print('Total number of classes:', len(classes))"]},{"cell_type":"markdown","id":"6jAImxzgCrMD","metadata":{"id":"6jAImxzgCrMD"},"source":["### Define a Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"id":"Uf9J_fBZ8Agx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uf9J_fBZ8Agx","outputId":"0ef8e85e-9d15-4b28-e2d4-c8ceaff3fded"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:03<00:00, 158MB/s]\n"]}],"source":["# Load the pre-trained VGG16 model with ImageNet weights\n","model_ft = models.vgg16(weights='IMAGENET1K_V1')\n","\n","# Freeze convolutional layers and the fully connected block except for its output layer\n","# Replace the latter to adapt to the new classification task\n","for param in model_ft.parameters():\n","    param.requires_grad = False\n","\n","# Get the number of input features for the final classifier layer\n","n_inputs = model_ft.classifier[6].in_features\n","\n","# Replace the last classifier layer with a custom fully connected layer\n","# This adapts the model to the new classification task with 5 output classes\n","model_ft.classifier[6] = nn.Linear(in_features=n_inputs, out_features=5)\n","\n","# Move the model to the specified device (CPU or GPU)\n","model_ft = model_ft.to(device)"]},{"cell_type":"markdown","id":"8N0mO7oM-yg4","metadata":{"id":"8N0mO7oM-yg4"},"source":["### Early Stop Function"]},{"cell_type":"code","execution_count":null,"id":"b01770eb-08c2-45f6-a808-afddf1f9d3e0","metadata":{"id":"b01770eb-08c2-45f6-a808-afddf1f9d3e0"},"outputs":[],"source":["# Define a function for Early Stopping.\n","class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        \"\"\"\n","        Class to manage early stopping during model training.\n","        Args:\n","          patience (int): Number of epochs with worsening validation loss to tolerate before stopping training.\n","          min_delta (float): Minimum improvement in validation loss considered significant.\n","          save_path (str): Path to save the model with the best validation loss.\n","        \"\"\"\n","        self.patience = patience # Number of epochs to wait before stopping if no improvement\n","        self.min_delta = min_delta # Minimum change in validation loss to be considered as an improvement\n","        self.counter = 0 # Counter to track how many epochs have passed without improvement\n","        self.min_validation_loss = float('inf') # Initialize the minimum validation loss as infinity\n","\n","    def early_stop(self, validation_loss):\n","        \"\"\"\n","        Control whether training should be stopped based on the current validation loss.\n","\n","        Args:\n","          validation_loss (float): The validation loss of the current epoch.\n","          model (torch.nn.Module): The model to save if the best validation loss is found.\n","        Returns:\n","          bool: True if training should be stopped, False otherwise.\n","        \"\"\"\n","        if validation_loss < self.min_validation_loss:\n","            # If the validation loss improves, update the minimum loss and reset counter\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            # If validation loss worsens beyond the minimum delta, increase the counter\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                # Stop training if the patience limit is reached\n","                return True\n","        return False # Continue training if conditions for stopping are not met"]},{"cell_type":"markdown","id":"w-i2xrDX-8fG","metadata":{"id":"w-i2xrDX-8fG"},"source":["### Hyperparameters Grid"]},{"cell_type":"code","execution_count":null,"id":"q3EQdo6PhaZU","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"q3EQdo6PhaZU","outputId":"dea597f2-998c-4cee-f917-18f63b47586b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with lr=0.001, batch_size=32, weight_decay=0.001, step_size=3\n","Epoch: 0 Train Accuracy: 0.7872174590802806 Train Loss: 0.6266056299209595 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.1496143341064453\n","Epoch: 1 Train Accuracy: 0.9431021044427124 Train Loss: 0.19932575523853302 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.11325333267450333\n","Epoch: 2 Train Accuracy: 0.9532346063912704 Train Loss: 0.17263907194137573 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.10012005269527435\n","Epoch: 3 Train Accuracy: 0.9540140296180826 Train Loss: 0.15043985843658447 Val. Accuracy: 0.9700272479564033 Val. Loss: 0.09694067388772964\n","Epoch: 4 Train Accuracy: 0.9579111457521434 Train Loss: 0.12537121772766113 Val. Accuracy: 0.989100817438692 Val. Loss: 0.05828523635864258\n","Epoch: 5 Train Accuracy: 0.9649259547934529 Train Loss: 0.107463039457798 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.06496129184961319\n","Epoch: 6 Train Accuracy: 0.9633671083398285 Train Loss: 0.11636640131473541 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.05439777672290802\n","Epoch: 7 Train Accuracy: 0.9727201870615745 Train Loss: 0.09127913415431976 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.06066756695508957\n","Epoch: 8 Train Accuracy: 0.9641465315666407 Train Loss: 0.10682734102010727 Val. Accuracy: 0.9782016348773842 Val. Loss: 0.07377524673938751\n","Epoch: 9 Train Accuracy: 0.9618082618862042 Train Loss: 0.10731885582208633 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.06539786607027054\n","Epoch: 10 Train Accuracy: 0.9734996102883866 Train Loss: 0.09136518836021423 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.0699077844619751\n","Epoch: 11 Train Accuracy: 0.9711613406079501 Train Loss: 0.08594995737075806 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.0442480593919754\n","Epoch: 12 Train Accuracy: 0.9750584567420109 Train Loss: 0.07343234121799469 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.056499797850847244\n","Epoch: 13 Train Accuracy: 0.9797349961028838 Train Loss: 0.06723308563232422 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.05778875574469566\n","Epoch: 14 Train Accuracy: 0.970381917381138 Train Loss: 0.08948616683483124 Val. Accuracy: 0.9700272479564033 Val. Loss: 0.07386147975921631\n","Epoch: 15 Train Accuracy: 0.9789555728760717 Train Loss: 0.07507690787315369 Val. Accuracy: 0.989100817438692 Val. Loss: 0.03684055060148239\n","Epoch: 16 Train Accuracy: 0.9758378799688231 Train Loss: 0.0745512917637825 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.046367865055799484\n","Epoch: 17 Train Accuracy: 0.9672642244738893 Train Loss: 0.08980314433574677 Val. Accuracy: 0.989100817438692 Val. Loss: 0.044011395424604416\n","Epoch: 18 Train Accuracy: 0.9734996102883866 Train Loss: 0.0702519565820694 Val. Accuracy: 0.989100817438692 Val. Loss: 0.0342382974922657\n","Epoch: 19 Train Accuracy: 0.9789555728760717 Train Loss: 0.06464413553476334 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.05045979842543602\n","Epoch: 20 Train Accuracy: 0.9781761496492596 Train Loss: 0.06315149366855621 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.03634808585047722\n","Epoch: 21 Train Accuracy: 0.9766173031956352 Train Loss: 0.06768564879894257 Val. Accuracy: 0.989100817438692 Val. Loss: 0.04303153231739998\n","Epoch: 22 Train Accuracy: 0.9773967264224473 Train Loss: 0.05926470831036568 Val. Accuracy: 0.989100817438692 Val. Loss: 0.03758779913187027\n","Epoch: 23 Train Accuracy: 0.9711613406079501 Train Loss: 0.07486941665410995 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.04988866299390793\n","Epoch: 24 Train Accuracy: 0.9734996102883866 Train Loss: 0.06790876388549805 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.06360844522714615\n","Epoch: 25 Train Accuracy: 0.9773967264224473 Train Loss: 0.06942503154277802 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.05883704870939255\n","Epoch: 26 Train Accuracy: 0.9758378799688231 Train Loss: 0.0734957680106163 Val. Accuracy: 0.9754768392370572 Val. Loss: 0.06566935777664185\n","Epoch: 27 Train Accuracy: 0.9836321122369447 Train Loss: 0.04994279518723488 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.056751422584056854\n","Epoch: 28 Train Accuracy: 0.9672642244738893 Train Loss: 0.07852087169885635 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.04121846333146095\n","Epoch: 29 Train Accuracy: 0.9812938425565082 Train Loss: 0.061819907277822495 Val. Accuracy: 0.9673024523160763 Val. Loss: 0.07558264583349228\n","Epoch: 30 Train Accuracy: 0.9734996102883866 Train Loss: 0.07608083635568619 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.03715933859348297\n","Epoch: 31 Train Accuracy: 0.9719407638347622 Train Loss: 0.07670727372169495 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.04599171131849289\n","Epoch: 32 Train Accuracy: 0.9719407638347622 Train Loss: 0.07097166776657104 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.05299066752195358\n","Epoch: 33 Train Accuracy: 0.9844115354637568 Train Loss: 0.05482522025704384 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.02610188163816929\n","Epoch: 34 Train Accuracy: 0.9789555728760717 Train Loss: 0.06358063220977783 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.0370967760682106\n","Epoch: 35 Train Accuracy: 0.9805144193296961 Train Loss: 0.06745479255914688 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.02435230277478695\n","Epoch: 36 Train Accuracy: 0.9688230709275136 Train Loss: 0.06948231905698776 Val. Accuracy: 0.9536784741144414 Val. Loss: 0.10044518113136292\n","Epoch: 37 Train Accuracy: 0.9625876851130164 Train Loss: 0.10928456485271454 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.054843150079250336\n","Epoch: 38 Train Accuracy: 0.9820732657833203 Train Loss: 0.053262289613485336 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.04249231144785881\n","Epoch: 39 Train Accuracy: 0.9766173031956352 Train Loss: 0.06667367368936539 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.0468890443444252\n","Epoch: 40 Train Accuracy: 0.9867498051441933 Train Loss: 0.04454837739467621 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.06840711832046509\n","Epoch: 41 Train Accuracy: 0.9875292283710054 Train Loss: 0.04768456518650055 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.04640163853764534\n","Epoch: 42 Train Accuracy: 0.9828526890101325 Train Loss: 0.04955571889877319 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.05695654824376106\n","Epoch: 43 Train Accuracy: 0.9851909586905689 Train Loss: 0.04774896055459976 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.0525868758559227\n","Epoch: 44 Train Accuracy: 0.9781761496492596 Train Loss: 0.05751437693834305 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.03788525611162186\n","Epoch: 45 Train Accuracy: 0.9711613406079501 Train Loss: 0.060275815427303314 Val. Accuracy: 0.9700272479564033 Val. Loss: 0.07711531221866608\n","Epoch: 46 Train Accuracy: 0.9766173031956352 Train Loss: 0.06972648203372955 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.06121129170060158\n","Epoch: 47 Train Accuracy: 0.9696024941543258 Train Loss: 0.08122313767671585 Val. Accuracy: 0.9754768392370572 Val. Loss: 0.0552491694688797\n","Epoch: 48 Train Accuracy: 0.9734996102883866 Train Loss: 0.0763266310095787 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.049753785133361816\n","Epoch: 49 Train Accuracy: 0.9789555728760717 Train Loss: 0.05965539067983627 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.04175039753317833\n","Epoch: 50 Train Accuracy: 0.9789555728760717 Train Loss: 0.06294428557157516 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.03858647122979164\n","Epoch: 51 Train Accuracy: 0.9781761496492596 Train Loss: 0.07062853127717972 Val. Accuracy: 0.989100817438692 Val. Loss: 0.029173925518989563\n","Epoch: 52 Train Accuracy: 0.9812938425565082 Train Loss: 0.0670749843120575 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.06657668203115463\n","Epoch: 53 Train Accuracy: 0.9773967264224473 Train Loss: 0.04866483435034752 Val. Accuracy: 0.9782016348773842 Val. Loss: 0.062341831624507904\n","Epoch: 54 Train Accuracy: 0.9844115354637568 Train Loss: 0.05287051573395729 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.03283637762069702\n","Epoch: 55 Train Accuracy: 0.9812938425565082 Train Loss: 0.0516364723443985 Val. Accuracy: 0.9809264305177112 Val. Loss: 0.0498325452208519\n","Epoch: 56 Train Accuracy: 0.9766173031956352 Train Loss: 0.07304535061120987 Val. Accuracy: 0.989100817438692 Val. Loss: 0.05852533131837845\n","Epoch: 57 Train Accuracy: 0.9797349961028838 Train Loss: 0.05946957319974899 Val. Accuracy: 0.989100817438692 Val. Loss: 0.038436081260442734\n","Epoch: 58 Train Accuracy: 0.9773967264224473 Train Loss: 0.07142768800258636 Val. Accuracy: 0.997275204359673 Val. Loss: 0.0266262274235487\n","Epoch: 59 Train Accuracy: 0.9789555728760717 Train Loss: 0.054103873670101166 Val. Accuracy: 0.989100817438692 Val. Loss: 0.02953428216278553\n","Epoch: 60 Train Accuracy: 0.9781761496492596 Train Loss: 0.056071698665618896 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.02543458715081215\n","Epoch: 61 Train Accuracy: 0.9805144193296961 Train Loss: 0.06171967461705208 Val. Accuracy: 0.9754768392370572 Val. Loss: 0.06503817439079285\n","Epoch: 62 Train Accuracy: 0.9773967264224473 Train Loss: 0.06143028661608696 Val. Accuracy: 0.989100817438692 Val. Loss: 0.03554229065775871\n","Epoch: 63 Train Accuracy: 0.9797349961028838 Train Loss: 0.06595999002456665 Val. Accuracy: 0.989100817438692 Val. Loss: 0.05640735104680061\n","Epoch: 64 Train Accuracy: 0.9758378799688231 Train Loss: 0.07853517681360245 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.06793767213821411\n","Epoch: 65 Train Accuracy: 0.9734996102883866 Train Loss: 0.07200226187705994 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.03817730396986008\n","Epoch: 66 Train Accuracy: 0.9773967264224473 Train Loss: 0.06729334592819214 Val. Accuracy: 0.9863760217983651 Val. Loss: 0.03567410632967949\n","Epoch: 67 Train Accuracy: 0.9742790335151987 Train Loss: 0.060191623866558075 Val. Accuracy: 0.989100817438692 Val. Loss: 0.030354458838701248\n","Epoch: 68 Train Accuracy: 0.9844115354637568 Train Loss: 0.051570694893598557 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.06064235046505928\n","Epoch: 69 Train Accuracy: 0.9789555728760717 Train Loss: 0.052567824721336365 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.05511123314499855\n","Epoch: 70 Train Accuracy: 0.9844115354637568 Train Loss: 0.04880578815937042 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.06869196891784668\n","Epoch: 71 Train Accuracy: 0.9797349961028838 Train Loss: 0.05703895166516304 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.08289048075675964\n","Epoch: 72 Train Accuracy: 0.9781761496492596 Train Loss: 0.05797097086906433 Val. Accuracy: 0.9945504087193461 Val. Loss: 0.03192540258169174\n","Epoch: 73 Train Accuracy: 0.9851909586905689 Train Loss: 0.04035237058997154 Val. Accuracy: 0.9754768392370572 Val. Loss: 0.08249717205762863\n","Epoch: 74 Train Accuracy: 0.9789555728760717 Train Loss: 0.05990464240312576 Val. Accuracy: 0.9918256130790191 Val. Loss: 0.03395919129252434\n","Training with lr=0.001, batch_size=32, weight_decay=0.001, step_size=10\n","Epoch: 0 Train Accuracy: 0.9789555728760717 Train Loss: 0.05071994289755821 Val. Accuracy: 0.9945504087193461 Val. Loss: 0.025942349806427956\n","Epoch: 1 Train Accuracy: 0.9797349961028838 Train Loss: 0.05257081612944603 Val. Accuracy: 0.9836512261580381 Val. Loss: 0.03158898279070854\n","Epoch: 2 Train Accuracy: 0.9750584567420109 Train Loss: 0.0626915991306305 Val. Accuracy: 0.9782016348773842 Val. Loss: 0.05980996415019035\n","Epoch: 3 Train Accuracy: 0.9758378799688231 Train Loss: 0.058517273515462875 Val. Accuracy: 0.989100817438692 Val. Loss: 0.036880187690258026\n","Epoch: 4 Train Accuracy: 0.9773967264224473 Train Loss: 0.056537121534347534 Val. Accuracy: 0.9727520435967303 Val. Loss: 0.11334005743265152\n"]}],"source":["# Define a grid of hyperparameters\n","learning_rates = [0.001, 0.0001] # Different learning rates to test\n","batch_sizes = [32, 64] # Different batch sizes to test\n","weight_decay= [0.001, 0.0001] # Different weight decay values to test\n","step_size= [3, 10] # Step size values for learning rate scheduler\n","\n","# Initialize variables to track the best model configuration\n","best_accuracy = 0.0 # Stores the highest validation accuracy achieved\n","best_hyperparams = {} # Dictionary to store the best hyperparameters\n","num_epochs=100 # Set the number of epochs for training\n","\n","# Store results for each combination\n","results = []\n","\n","# Iterate over all possible combinations of hyperparameters\n","for lr, batch_size, weight_decay, step_size, in itertools.product(learning_rates, batch_sizes, weight_decay, step_size):\n","    print(f\"Training with lr={lr}, batch_size={batch_size}, weight_decay={weight_decay}, step_size={step_size}\")\n","\n","    # Create data loaders with the current batch size\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True) # Training data loader\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False) # Validation data loader\n","\n","    # Re-initialize the optimizer with the current learning rate\n","    optimizer_ft = Adam(model_ft.classifier.parameters(), lr=lr, weight_decay=weight_decay)\n","    loss_function_ft = nn.CrossEntropyLoss() # Loss function for multi-class classification\n","    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=step_size, gamma=0.1) # Learning rate scheduler\n","    early_stopper = EarlyStopper(patience=5, min_delta=10) # Initialize early stopping mechanism\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model_ft.train() # Set model to training mode\n","        train_accuracy = 0.0\n","        train_loss = 0.0\n","\n","        for images, labels in train_loader: # Iterate over training batches\n","            images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n","            optimizer_ft.zero_grad() # Reset gradients\n","            outputs = model_ft(images) # Forward pass\n","            loss = loss_function_ft(outputs, labels) # Compute loss\n","            loss.backward() # Backpropagate\n","            optimizer_ft.step() # Update weights\n","\n","            train_loss += loss.cpu().data * images.size(0) # Accumulate training loss\n","            _, prediction = torch.max(outputs.data, 1) # Get predicted class labels\n","            train_accuracy += int(torch.sum(prediction == labels.data)) # Count correct predictions\n","\n","        train_accuracy /= len(train_data) # Compute training accuracy\n","        train_loss /= len(train_data) # Compute average training loss\n","\n","        # Evaluate the model on validation set\n","        model_ft.eval() # Set model to evaluation mode\n","        val_accuracy = 0.0\n","        val_loss = 0.0\n","\n","        with torch.no_grad(): # Disable gradient computation for validation\n","            for images, labels in val_loader: # Iterate over validation batches\n","                images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n","                outputs = model_ft(images) # Forward pass\n","                loss = loss_function_ft(outputs, labels) # Compute validation loss\n","                val_loss += loss.cpu().data * images.size(0) # Accumulate validation loss\n","                _, prediction = torch.max(outputs.data, 1) # Get predicted class labels\n","                val_accuracy += int(torch.sum(prediction == labels.data)) # Count correct predictions\n","\n","        val_accuracy /= len(val_data) # Compute validation accuracy\n","        val_loss /= len(val_data) # Compute average validation loss\n","\n","        # Print training and validation metrics\n","        print(f'Epoch: {epoch} Train Accuracy: {train_accuracy} Train Loss: {train_loss} Val. Accuracy: {val_accuracy} Val. Loss: {val_loss}')\n","\n","        # Check early stopping condition\n","        if early_stopper.early_stop(val_loss):\n","            break # Stop training if validation loss does not improve\n","\n","        # Save the model if it achieves the best validation accuracy so far\n","        if val_accuracy > best_accuracy:\n","            torch.save(model_ft.state_dict(), 'best_checkpoint_ft.model')  # Save the model state\n","            best_accuracy = val_accuracy # Update best accuracy\n","            best_hyperparams = {'lr': lr, 'batch_size': batch_size, 'num_epochs': num_epochs} # Store best hyperparameters\n","\n","    # Append results for the current combination of hyperparameters\n","    results.append((lr, batch_size, num_epochs, val_accuracy))\n","\n","# Print the best hyperparameter configuration found\n","print(f'Best hyperparameters: {best_hyperparams} with validation accuracy: {best_accuracy}')"]},{"cell_type":"markdown","id":"0LgieGq5lG6S","metadata":{"id":"0LgieGq5lG6S"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"id":"sbilSYjnlJoG","metadata":{"id":"sbilSYjnlJoG"},"outputs":[],"source":["# Initialization of lists to store training and validation metrics\n","summary_loss_train = [] # Stores training loss for each epoch\n","summary_acc_train = [] # Stores training accuracy for each epoch\n","summary_loss_val = [] # Stores validation loss for each epoch\n","summary_acc_val = [] # Stores validation accuracy for each epoch\n","summary_precision_train = [] # Stores training precision for each epoch\n","summary_recall_train = [] # Stores training recall for each epoch\n","summary_f1_train = [] # Stores training F1-score for each epoch\n","summary_precision_val = [] # Stores validation precision for each epoch\n","summary_recall_val = [] # Stores validation recall for each epoch\n","summary_f1_val = [] # Stores validation F1-score for each epoch\n","\n","# Load the pre-trained VGG16 model with ImageNet weights\n","model_ft = models.vgg16(weights='IMAGENET1K_V1')\n","\n","# Freeze all layers of the pre-trained model\n","for param in model_ft.parameters():\n","    param.requires_grad = False # Prevents updates to pre-trained weights\n","\n","# Modify the final fully connected layer for the classification task\n","n_inputs = model_ft.classifier[6].in_features # Get the input size of the final layer\n","model_ft.classifier[6] = nn.Linear(in_features=n_inputs, out_features=5) # Replace last layer with 5 output neurons\n","model_ft = model_ft.to(device) # Move model to GPU if available\n","\n","# Store results for each combination\n","results = []\n","\n","# Create DataLoaders with the current batch size\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True) # Training data loader\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False) # Validation data loader\n","\n","# Re-initialize the optimizer with the defined hyperparameters\n","optimizer_ft = Adam(model_ft.classifier.parameters(), lr=0.01, weight_decay=0.001) # Adam optimizer\n","loss_function_ft = nn.CrossEntropyLoss() # Cross-entropy loss function for classification\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=8, gamma=0.1) # Learning rate scheduler\n","early_stopper = EarlyStopper(patience=5, min_delta=10) # Early stopping mechanism\n","\n","best_accuracy = 0.0 # Track the best validation accuracy\n","\n","# Training loop for 70 epochs\n","for epoch in range(70):\n","    model_ft.train() # Set model to training mode\n","    train_accuracy = 0.0\n","    train_loss = 0.0\n","    all_train_labels = []\n","    all_train_preds = []\n","\n","    for images, labels in train_loader: # Iterate through training batches\n","        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n","        optimizer_ft.zero_grad() # Reset gradients\n","        outputs = model_ft(images) # Forward pass\n","        loss = loss_function_ft(outputs, labels) # Compute loss\n","        loss.backward() # Backpropagation\n","        optimizer_ft.step() # Update weights\n","\n","        train_loss += loss.cpu().data * images.size(0) # Accumulate loss\n","        _, prediction = torch.max(outputs.data, 1) # Get predicted class\n","        train_accuracy += int(torch.sum(prediction == labels.data)) # Count correct predictions\n","        all_train_labels.extend(labels.cpu().numpy()) # Store true labels\n","        all_train_preds.extend(prediction.cpu().numpy()) # Store predicted labels\n","\n","    train_accuracy /= len(train_data) # Compute training accuracy\n","    train_loss /= len(train_data) # Compute average training loss\n","\n","    # Compute precision, recall, and F1-score for training data\n","    train_precision = precision_score(all_train_labels, all_train_preds, average='macro')\n","    train_recall = recall_score(all_train_labels, all_train_preds, average='macro')\n","    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n","\n","    # Validation phase\n","    model_ft.eval() # Set model to evaluation mode\n","    val_accuracy = 0.0\n","    val_loss = 0.0\n","    all_val_labels = []\n","    all_val_preds = []\n","\n","    with torch.no_grad(): # Disable gradient computation for validation\n","        for images, labels in val_loader: # Iterate through validation batches\n","            images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n","            outputs = model_ft(images) # Forward pass\n","            loss = loss_function_ft(outputs, labels) # Compute validation loss\n","            val_loss += loss.cpu().data * images.size(0) # Accumulate loss\n","            _, prediction = torch.max(outputs.data, 1) # Get predicted class\n","            val_accuracy += int(torch.sum(prediction == labels.data)) # Count correct predictions\n","            all_val_labels.extend(labels.cpu().numpy()) # Store true labels\n","            all_val_preds.extend(prediction.cpu().numpy()) # Store predicted labels\n","\n","    val_accuracy /= len(val_data) # Compute validation accuracy\n","    val_loss /= len(val_data) # Compute average validation loss\n","\n","    # Compute precision, recall, and F1-score for validation data\n","    val_precision = precision_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n","    val_recall = recall_score(all_val_labels, all_val_preds, average='macro')\n","    val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n","\n","    # Store training and validation metrics\n","    summary_loss_train.append(train_loss.item())\n","    summary_acc_train.append(train_accuracy)\n","    summary_precision_train.append(train_precision)\n","    summary_recall_train.append(train_recall)\n","    summary_f1_train.append(train_f1)\n","\n","    summary_loss_val.append(val_loss.item())\n","    summary_acc_val.append(val_accuracy)\n","    summary_precision_val.append(val_precision)\n","    summary_recall_val.append(val_recall)\n","    summary_f1_val.append(val_f1)\n","\n","    # Print training and validation results for the current epoch\n","    print(f'Epoch: {epoch} Train Accuracy: {train_accuracy:.4f} Train Loss: {train_loss:.4f} '\n","          f'Val. Accuracy: {val_accuracy:.4f} Val. Loss: {val_loss:.4f} '\n","          f'Train Precision: {train_precision:.4f} Train Recall: {train_recall:.4f} Train F1: {train_f1:.4f} '\n","          f'Val Precision: {val_precision:.4f} Val Recall: {val_recall:.4f} Val F1: {val_f1:.4f}')\n","\n","    # Apply early stopping if validation loss does not improve\n","    if early_stopper.early_stop(val_loss):\n","        break # Stop training\n","\n","    # Save the model if it achieves the best validation accuracy so far\n","    if val_accuracy > best_accuracy:\n","        torch.save(model_ft.state_dict(), 'best_checkpoint_ft.model') # Save model state\n","        best_accuracy = val_accuracy # Update best validation accuracy\n","\n","# Print the highest validation accuracy achieved\n","print(f'Best validation accuracy: {best_accuracy}')"]},{"cell_type":"markdown","id":"cyhhyFDWF9Oi","metadata":{"id":"cyhhyFDWF9Oi"},"source":["Plots"]},{"cell_type":"code","execution_count":null,"id":"XB8jee2k_nsv","metadata":{"id":"XB8jee2k_nsv"},"outputs":[],"source":["# Convert the summary_loss_val tensor to a CPU tensor\n","summary_loss_val_cpu= torch.tensor(summary_loss_val, device = 'cpu')\n","\n","# Convert the CPU tensor to a Python list\n","summary_loss_val_cpu_lt=list(summary_loss_val_cpu)\n","print(summary_loss_val_cpu_lt) # Print the converted validation loss list\n","\n","# Create a figure with two subplots side by side\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15,6)) # Set figure size and create two subplots\n","\n","# Create a list of integers from 0 to 100 to use as the x-axis values (assuming 100 epochs)\n","x = [i for i in range(100)]\n","\n","# Convert summary_acc_train to a list\n","sommario_acc_train_array = []\n","for idx in range(len(summary_acc_train)):\n","    sommario_acc_train_array.append(summary_acc_train[idx]) # Append accuracy values from training\n","\n","# Convert summary_acc_train to a list\n","sommario_acc_val_array = [] # Initialize an empty list\n","for idx in range(len(summary_acc_val)):\n","    sommario_acc_val_array.append(summary_acc_val[idx]) # Append accuracy values from training\n","\n","# Plot training and validation loss on the first subplot (ax1)\n","ax1.plot(x, summary_loss_train [:100], label = 'Training Loss') # Plot training loss\n","ax1.plot(x, summary_loss_val_cpu_lt [:100], label = 'Validation Loss') # Plot validation loss\n","ax1.legend() # Add legend to distinguish the curves\n","\n","# Plot training and validation accuracy on the second subplot (ax2)\n","ax2.set_title(\"Accuracy\") # Set title for accuracy plot\n","ax2.plot(x, sommario_acc_train_array, label='Training Accuracy') # Plot training accuracy\n","ax2.plot(x, sommario_acc_val_array, label='Validation Accuracy') # Plot validation accuracy\n","ax2.legend() # Add legend\n","\n","# Alternative way to plot accuracy with a different length\n","#ax2.plot(x, summary_acc_train [:35], label='Training Accuracy')\n","#ax2.plot(x, summary_acc_val [:35], label='Validation Accuracy')\n","#ax2.legend()\n","\n","# Display the plots\n","plt.show()"]},{"cell_type":"markdown","id":"qVmxq-WaF05-","metadata":{"id":"qVmxq-WaF05-"},"source":["### Inference\n","We need to evaluate the trained network on the test dataset to determine how well it has learned. Although the model has undergone multiple training iterations, we must verify its effectiveness. This is done by making predictions using the trained network and comparing the predicted class labels with the actual ground-truth labels. If the prediction matches the correct label, the sample is counted as a correct prediction. This evaluation helps assess the model's generalization ability on unseen data."]},{"cell_type":"code","execution_count":null,"id":"K9PbHVAcgBGW","metadata":{"id":"K9PbHVAcgBGW"},"outputs":[],"source":["# Importing test dataset\n","class ImageDataset(Dataset):\n","    \"\"\"\n","    Custom dataset for loading images and their labels from a specified directory.\n","\n","    Attributes:\n","      root (str): Directory where the images are stored.\n","      transform (callable, optional): Optional transform to be applied on a sample.\n","      images (list): List of file paths for all images in the directory.\n","      labels (list): List of labels corresponding to each image.\n","    \"\"\"\n","\n","    def __init__(self, root, transform=None):\n","        \"\"\"\n","        Initialize the dataset with the directory containing images and optional transformations.\n","\n","        Args:\n","          root (str): Directory containing image files.\n","          transform (callable, optional): A function/transform that takes in an image and returns a transformed version.\n","        \"\"\"\n","        self.root = root # Store the root directory\n","        self.transform = transform # Store the transformation function\n","        self.images = [os.path.join(root, file) for file in os.listdir(root)] # List of all image file paths\n","        self.labels = [self.get_label(file) for file in os.listdir(root)] # Extract labels from filenames\n","\n","    def __len__(self):\n","        \"\"\"\n","        Return the total number of images in the dataset.\n","\n","        Returns:\n","          int: Number of images.\n","        \"\"\"\n","        return len(self.images) # Return the total number of images\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieve an image and its label at the given index, and optionally apply transformations.\n","\n","        Args:\n","          idx (int): Index of the image to retrieve.\n","\n","        Returns:\n","          tuple: Transformed image and its label as a tensor.\n","        \"\"\"\n","        img_path = self.images[idx] # Get the image file path\n","        image = Image.open(img_path).convert(\"RGB\") # Open image and convert to RGB format\n","\n","        if self.transform:\n","            image = self.transform(image) # Apply transformations if specified\n","\n","        label = self.labels[idx] # Retrieve the corresponding label\n","        return image, torch.tensor(label) # Return the image and label as a tensor\n","\n","    def get_label(self, filename):\n","        \"\"\"\n","        Extract the label from the filename based on predefined class names.\n","\n","        Args:\n","          filename (str): Name of the image file.\n","\n","        Returns:\n","          int: Numeric label corresponding to the class name.\n","        \"\"\"\n","        # Define the regex pattern for matching class names\n","        patterns = r'CMC|COR|HUM|TMT|TT'\n","\n","        # Find the first match of the pattern in the filename\n","        match = re.search(patterns, filename)\n","\n","        # Define the mapping from class names to indices\n","        class_to_idx = {'CMC': 0, 'COR': 1, 'HUM': 2, 'TMT': 3, 'TT': 4}\n","\n","        if match:\n","          class_name = match.group(0)  # Extract the matched class name\n","          return class_to_idx[class_name] # Return the corresponding label index\n","        else:\n","          raise ValueError(\"Class name not found in the filename\") # Raise an error if no class name is found\n","\n","# Loading dataset for testing\n","\n","# Path to test directory\n","test_path =  os.path.join(data_dir, 'bones_test')\n","\n","# If images are divided into different folders, use ImageFolder\n","# test_data = dsets.ImageFolder(root=test_path, transform=val_transform)\n","# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# If images are all in a single directory without class subfolders, use the custom ImageDataset (replace with your path)\n","test_dataset = ImageDataset(root='/tmp/bones_test', transform= val_transform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2, persistent_workers=True)\n","\n","# Count the number of test images\n","test_count=len(glob.glob(test_path+'/*.jpg'))\n","print(test_count) # Print the total number of test images"]},{"cell_type":"code","execution_count":null,"id":"2SmOzZ31Jeq8","metadata":{"id":"2SmOzZ31Jeq8"},"outputs":[],"source":["# Load the checkpoint from the specified file\n","checkpoint = torch.load('best_checkpoint_ft.model')  # Load saved model weights from the checkpoint file\n","\n","# Initialize a VGG16 model with pre-trained ImageNet weights\n","model_ft = models.vgg16(weights='IMAGENET1K_V1')  # Load pre-trained VGG16 model\n","\n","# Modify the classifier to have 5 output classes\n","n_inputs = model_ft.classifier[6].in_features  # Get the number of input features for the last layer\n","model_ft.classifier[6] = nn.Linear(n_inputs, 5)  # Replace the last layer to match the number of classes\n","\n","# Load the model state (weights) from the checkpoint into the initialized model\n","model_ft.load_state_dict(checkpoint)  # Restore the saved model parameters"]},{"cell_type":"code","execution_count":null,"id":"89obBBFIEKdt","metadata":{"id":"89obBBFIEKdt"},"outputs":[],"source":["# Calculate overall metrics for the test set\n","\n","model_ft.eval() # Set the model to evaluation mode (disables dropout and batch normalization)\n","test_accuracy = 0.0 # Initialize test accuracy counter\n","test_loss = 0.0 # Initialize test loss counter\n","all_test_labels = [] # List to store all ground-truth labels\n","all_test_preds = [] # List to store all predicted labels\n","\n","with torch.no_grad(): # Disable gradient computation for efficiency\n","    for images, labels in test_loader: # Iterate over test dataset batches\n","        images, labels = images.to(device), labels.to(device) # Move data to GPU/CPU\n","\n","        outputs = model_ft.to(device)(images) # Perform forward pass\n","\n","        loss = loss_function_ft(outputs, labels) # Compute loss\n","        test_loss += loss.cpu().data * images.size(0) # Accumulate total test loss\n","\n","        _, prediction = torch.max(outputs.data, 1) # Get predicted class with highest probability\n","        test_accuracy += int(torch.sum(prediction == labels.data)) # Count correct predictions\n","\n","        all_test_labels.extend(labels.cpu().numpy()) # Store true labels\n","        all_test_preds.extend(prediction.cpu().numpy()) # Store predicted labels\n","\n","# Compute average accuracy and loss over the entire test dataset\n","test_accuracy /= len(test_dataset)\n","test_loss /= len(test_dataset)\n","\n","# Compute additional classification metrics\n","test_precision = precision_score(all_test_labels, all_test_preds, average='macro') # Compute precision\n","test_recall = recall_score(all_test_labels, all_test_preds, average='macro') # Compute recall\n","test_f1 = f1_score(all_test_labels, all_test_preds, average='macro') # Compute F1-score\n","\n","# Print test performance metrics\n","print(f'Test Accuracy: {test_accuracy:.4f} Test Loss: {test_loss:.4f} '\n","      f'Test Precision: {test_precision:.4f} Test Recall: {test_recall:.4f} Test F1: {test_f1:.4f}')\n","\n","# Print the classification report and confusion matrix for the test set\n","print('Classification Report:')\n","print(classification_report(all_test_labels, all_test_preds, target_names=classes)) # Generate detailed report\n","\n","print('Confusion Matrix:')\n","print(confusion_matrix(all_test_labels, all_test_preds)) # Print confusion matrix"]},{"cell_type":"code","execution_count":null,"id":"RYTlsLC3MklD","metadata":{"id":"RYTlsLC3MklD"},"outputs":[],"source":["# Confusion matrix plot\n","\n","cm = confusion_matrix(all_test_labels, all_test_preds) # Compute confusion matrix\n","classes = ['CMC', 'COR', 'HUM', 'TMT', 'TT'] # Class labels for the confusion matrix\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Greens):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","\n","    Args:\n","      cm (array): Confusion matrix data.\n","      classes (list): List of class labels.\n","      normalize (bool): Whether to normalize the confusion matrix.\n","      title (str): Title of the plot.\n","      cmap: Colormap for the plot.\n","\n","    If `normalize=True`, the confusion matrix will be displayed as percentages.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # Normalize by row (true labels)\n","        print(\"Normalized Confusion Matrix\")\n","    else:\n","        print(\"Confusion Matrix, without Normalization\")\n","\n","    plt.figure(figsize=(7, 5)) # Set figure size\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap) # Display confusion matrix as an image\n","    plt.title(title) # Set title\n","    plt.colorbar() # Add color bar to indicate values\n","\n","    tick_marks = np.arange(len(classes)) # Get tick positions for class labels\n","    plt.xticks(tick_marks, classes, rotation=45) # Set class names on x-axis\n","    plt.yticks(tick_marks, classes) # Set class names on x-axis\n","\n","    # Define text format and threshold for text color contrast\n","    fmt = '.2f' if normalize else 'd' # Format numbers as decimals if normalized, otherwise as integers\n","    thresh = cm.max() / 2. # Set threshold for text color contrast\n","\n","    # Iterate through confusion matrix values and display them in the plot\n","    for i, j in np.ndindex(cm.shape):\n","        plt.text(j, i, format(cm[i, j], fmt), # Display text in each cell\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\") # Use contrasting colors for better visibility\n","\n","    plt.tight_layout() # Adjust layout for better fit\n","    plt.ylabel('True Label') # Label for y-axis\n","    plt.xlabel('Predicted Label') # Label for x-axis\n","    plt.show() # Display the plot\n","\n","# Plot non-normalized confusion matrix\n","plot_confusion_matrix(cm, classes, normalize=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":5}
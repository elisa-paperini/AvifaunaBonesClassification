{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2mfa_alsHMs_",
   "metadata": {
    "id": "2mfa_alsHMs_"
   },
   "source": [
    "**Taxonomic Classification with ResNet-101 and Random Forest**\n",
    "\n",
    "**Author**: Elisa Paperini, Nevio Dubbini\n",
    "\n",
    "**License**: CC-BY-SA 4.0\n",
    "\n",
    "**Year**: 2024 (last version)\n",
    "\n",
    "**Description**\n",
    "\n",
    "This script implements a taxonomic classification model using a pre-trained ResNet-101 network with IMAGENET1K_V2 weights.\n",
    "The final fully connected layer is replaced with a custom module:\n",
    "\n",
    "- Fully connected layer → Batch normalization → ReLU activation → Dropout\n",
    "- Final fully connected layer\n",
    "\n",
    "To enhance performance, a Random Forest classifier was integrated to classify the CNN-extracted features and cross-validation was applied. A grid search was used to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heQ--wRfHLw3",
   "metadata": {
    "id": "heQ--wRfHLw3"
   },
   "source": [
    "# Set the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81178d71",
   "metadata": {
    "id": "81178d71"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import copy  # Used for creating deep copies of objects\n",
    "import numpy as np  # Library for numerical computations\n",
    "import time  # Module for measuring execution time\n",
    "import os  # Provides operating system dependent functionalities\n",
    "from tempfile import TemporaryDirectory  # Creates temporary directories for file handling\n",
    "\n",
    "# Import libraries for Random Forest classification (from scikit-learn)\n",
    "from sklearn.metrics import classification_report  # Generates detailed classification reports\n",
    "from sklearn.ensemble import RandomForestClassifier  # Implements the Random Forest algorithm\n",
    "from sklearn.metrics import accuracy_score  # Computes classification accuracy\n",
    "from sklearn import metrics  # Collection of metrics for model evaluation\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV  # Splits data into training and test sets, K-fold, grisearch\n",
    "\n",
    "# Import PyTorch and TorchVision for deep learning\n",
    "import torch  # Main PyTorch library\n",
    "import torch.nn as nn  # Neural network layers\n",
    "import torch.nn.functional as F  # Functional operations for neural networks\n",
    "import torch.backends.cudnn as cudnn  # Backend optimizations for CUDA\n",
    "import torch.utils.data  # Data loading utilities\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torch.optim import lr_scheduler  # Learning rate scheduling\n",
    "from torchsummary import summary  # Model summary utility\n",
    "\n",
    "import torchvision  # Image processing and pre-trained models\n",
    "import torchvision.transforms as transforms  # Data transformations for image preprocessing\n",
    "from torchvision import datasets, models  # Pre-built datasets and models\n",
    "from torchvision.transforms.transforms import ColorJitter  # Color augmentation for images\n",
    "from torchvision.transforms.functional import perspective  # Perspective transformation for images\n",
    "\n",
    "# Import libraries for image processing and visualization\n",
    "import matplotlib.pyplot as plt  # Plotting utilities\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "from PIL import Image  # Image processing with Python Imaging Library (PIL)\n",
    "from PIL import ImageFile  # Handles truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Prevents errors caused by incomplete image files\n",
    "\n",
    "# Import pandas and CSV utilities for handling structured data\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import csv  # CSV file handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804dbb74",
   "metadata": {
    "id": "804dbb74"
   },
   "outputs": [],
   "source": [
    "# Checking for device availability (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use CUDA if available, otherwise default to CPU\n",
    "\n",
    "# Clear CUDA cache to free up memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print memory usage summary (uncomment if needed)\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "# Display the selected device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dToRAEIZYb",
   "metadata": {
    "id": "a9dToRAEIZYb"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e22399",
   "metadata": {
    "id": "43e22399"
   },
   "outputs": [],
   "source": [
    "# Data transformations for training and validation\n",
    "\n",
    "# Transformations applied to training data (includes augmentations for better generalization)\n",
    "data_transforms_t = transforms.Compose([\n",
    "                    transforms.Resize(size=(586,850)), # Resize images to 586x850 pixels\n",
    "                    transforms.RandomRotation(10), # Randomly rotate images by ±10 degrees\n",
    "                    transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=0, hue=0.1), # Adjust brightness, contrast, and hue\n",
    "                    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.5)), # Apply slight blurring for noise reduction\n",
    "                    transforms.ToTensor(), # Convert image to PyTorch tensor and scale pixel values to [0,1]\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalize with ImageNet mean and std\n",
    "                    ])\n",
    "\n",
    "# Transformations applied to validation data (no augmentations, only resizing and normalization)\n",
    "data_transforms_v = transforms.Compose([\n",
    "                    transforms.Resize(size=(586,850)), # Resize images to 586x850 pixels\n",
    "                    transforms.ToTensor(), # Convert image to PyTorch tensor\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalize using ImageNet statistics\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a88a9b",
   "metadata": {
    "id": "b0a88a9b"
   },
   "outputs": [],
   "source": [
    "# Define the data directory (change with your path)\n",
    "data_dir = 'C:\\\\Users\\\\Elisa\\\\Desktop\\\\dataset_ossa\\\\bones_detection_species'\n",
    "\n",
    "# Define the batch size (max limit set to 8 to prevent CUDA Out of Memory errors)\n",
    "batch_size_real = 8 # If set too high, it may cause CUDA OOM (Out of Memory)\n",
    "\n",
    "# Define paths for training and validation datasets\n",
    "training_dir = os.path.join(data_dir, 'train') # Path to training dataset\n",
    "validation_dir =  os.path.join(data_dir, 'val') # Path to validation dataset\n",
    "\n",
    "# Load datasets using ImageFolder (applies transformations defined earlier)\n",
    "training_dset = torchvision.datasets.ImageFolder(training_dir,data_transforms_t) # Training dataset with augmentations\n",
    "validation_dset = torchvision.datasets.ImageFolder(validation_dir,data_transforms_v) # Validation dataset (only normalization)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "training_loader = torch.utils.data.DataLoader(training_dset, batch_size=batch_size_real,\n",
    "                                             shuffle=True, num_workers=2) # Enable shuffling for training\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dset, batch_size=batch_size_real,\n",
    "                                             shuffle=True, num_workers=2) # Enable shuffling for validation\n",
    "\n",
    "# Extract class names from datasets\n",
    "class_names_tr = training_dset.classes # Training set class names\n",
    "class_names_va = validation_dset.classes # Validation set class names\n",
    "\n",
    "# Create an iterator for the training DataLoader\n",
    "dataiter = iter(training_loader) # Allows retrieving batches from the DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ud0_rDIAMMPM",
   "metadata": {
    "id": "ud0_rDIAMMPM"
   },
   "source": [
    "# CNN Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbaa80",
   "metadata": {
    "id": "eecbaa80"
   },
   "outputs": [],
   "source": [
    "#  Model definition, useful for potential debugging\n",
    "\n",
    "# Load a pre-trained ResNet-101 model with default weights\n",
    "model_ft = models.resnet101(weights='DEFAULT')\n",
    "\n",
    "# Get the number of input features for the fully connected (fc) layer\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Replace the default fully connected layer with a new one matching the number of classes\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names_tr))\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "model = model_ft.to(device)\n",
    "\n",
    "# Define a custom fully connected module for classification\n",
    "class CustomModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom classification head for ResNet-101.\n",
    "\n",
    "    Args:\n",
    "      num_ftrs (int): Number of input features.\n",
    "      num_classes (int): Number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_ftrs, num_classes):\n",
    "        super(CustomModule, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_ftrs, 512) # First fully connected layer (reduces features to 512)\n",
    "        self.norm = nn.BatchNorm1d(512) # Batch normalization for stabilization\n",
    "        self.relu = nn.ReLU() # ReLU activation function\n",
    "        self.dropout = nn.Dropout(0.5) # Dropout layer (50%) to prevent overfitting\n",
    "        self.layer2 = nn.Linear(512, num_classes) # Final fully connected layer (maps to class outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the custom module.\n",
    "\n",
    "        Args:\n",
    "          x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "          Tensor: Output logits for classification.\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def get_layer(self, layer_name):\n",
    "        \"\"\"\n",
    "        Retrieve a specific layer by name (useful for debugging).\n",
    "\n",
    "        Args:\n",
    "          layer_name (str): Name of the layer to retrieve.\n",
    "\n",
    "        Returns:\n",
    "          nn.Module or None: The requested layer if it exists, otherwise None.\n",
    "        \"\"\"\n",
    "        return getattr(self, layer_name, None)\n",
    "\n",
    "\n",
    "# Replace ResNet's final classification head with the custom module\n",
    "model.fc = CustomModule(num_ftrs, len(class_names_tr)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db6638",
   "metadata": {
    "id": "78db6638"
   },
   "outputs": [],
   "source": [
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7a7ce",
   "metadata": {
    "id": "b1a7a7ce"
   },
   "outputs": [],
   "source": [
    "# Define a function to reset model weights before each training cycle\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    This function iterates through each layer of the model and resets its weights before training.\n",
    "    It should be called before training the model, specifically before `model.train()`.\n",
    "\n",
    "    Usage:\n",
    "      model.apply(init_weights)  # Resets all model weights\n",
    "\n",
    "    Args:\n",
    "      m (nn.Module): A model layer to be checked and reinitialized.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        # Reinitialize weights for embedding layers with normal distribution\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.1)\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Reinitialize weights for fully connected (linear) layers with Xavier initialization\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=np.sqrt(1 / m.in_features))\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias) # Set biases to zero\n",
    "\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        # Reinitialize weights for 1D convolutional layers with a variance-scaled normal distribution\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=np.sqrt(4 / m.in_channels))\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias) # Set biases to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea6971",
   "metadata": {
    "id": "22ea6971"
   },
   "outputs": [],
   "source": [
    "def reinitialize_layers(module):\n",
    "    \"\"\"\n",
    "    Reinitialize the weights of the given module using Xavier initialization.\n",
    "\n",
    "    This function resets the weights of convolutional (`Conv2d`) and fully connected (`Linear`) layers\n",
    "    to ensure proper initialization before training, which can help improve model convergence.\n",
    "\n",
    "    Args:\n",
    "      module (nn.Module): The layer to be reinitialized.\n",
    "\n",
    "    Usage:\n",
    "      model.apply(reinitialize_layers)  # Resets weights for all applicable layers\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight) # Apply Xavier uniform initialization\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.fill_(0.01) # Set bias values to a small constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f5e9a",
   "metadata": {
    "id": "3d9f5e9a",
    "outputId": "30b65a46-8f75-4623-e450-ac8858b11fb6"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Grid Search and Model Training\n",
    "import itertools # Used to iterate over all hyperparameter combinations\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Define loss function\n",
    "epoch = 50 # Number of training epochs\n",
    "\n",
    "# Define hyperparameter grid\n",
    "batch_size = [4, 32, 64] # Small batch size to avoid memory issues\n",
    "learning_rate = [0.01, 0.001, 0.0001] # Learning rate for optimization\n",
    "momentum = [0.40, 0.70, 0.90] # Momentum for the optimizer\n",
    "\n",
    "# Initialize lists to store training and validation statistics\n",
    "summary_loss_train = [] # Store training loss per epoch\n",
    "summary_acc_train = [] # Store training accuracy per epoch\n",
    "summary_loss_val = [] # Store validation loss per epoch\n",
    "summary_acc_val = [] # Store validation accuracy per epoch\n",
    "\n",
    "best_accuracy = 0.0 # Track the best validation accuracy achieved\n",
    "best_hyperparameters = {} # Store the best hyperparameter combination\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict()) # Save the best model weights\n",
    "best_acc = 0.0 # Best validation accuracy\n",
    "\n",
    "since = time.time() # Track training start time\n",
    "\n",
    "# Create a dataframe to store hyperparameter results\n",
    "df_hyperparameters = pd.DataFrame(columns=['epoca', 'learning_rate', 'batch_size', 'momentum', 'training_acc', 'validation_acc'])\n",
    "\n",
    "# Create a CSV file to store hyperparameter search results\n",
    "i_train = 0\n",
    "fields = ['epoca', 'learning_rate', 'batch_size', 'momentum', 'training_acc', 'validation_acc']\n",
    "\n",
    "with open('hyperparameters_grid.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fields)\n",
    "    writer.writeheader()\n",
    "    csvfile.close()\n",
    "\n",
    "# Model Training Loop with Hyperparameter Grid Search\n",
    "for lr, batch_size, momentum in itertools.product(learning_rate, batch_size, momentum):\n",
    "\n",
    "    # Initialize a new optimizer with the current hyperparameters\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size = 6, gamma = 0.1) # Reduce learning rate every 6 epochs\n",
    "\n",
    "    # Reinitialize model weights before training\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        i_train += 1\n",
    "        ### TRAINING PHASE ###\n",
    "        model.train() # Set model to training mode\n",
    "        loss_current_train = 0.0 # Track training loss\n",
    "        acc_current_train = 0.0 # Track training accuracy\n",
    "\n",
    "        for inputs, labels in training_loader:\n",
    "            model = model.to(device) # Ensure model is on the correct device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the selected device\n",
    "\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            loss = criterion(outputs, labels) # Compute loss\n",
    "\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step() # Update model parameters\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) # Get predicted class\n",
    "            loss_current_train += loss.item() * inputs.size(0) # Accumulate training loss\n",
    "            acc_current_train += torch.sum(preds == labels.data) # Count correct predictions\n",
    "\n",
    "        epoch_acc = acc_current_train.float() / len(training_dset) # Compute training accuracy\n",
    "\n",
    "        # Adjust learning rate dynamically\n",
    "        scheduler.step()\n",
    "\n",
    "        ### VALIDATION PHASE ###\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        loss_current_val = 0.0 # Track validation loss\n",
    "        acc_current_val = 0.0 # Track validation accuracy\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient computation during validation\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  # Move data to device\n",
    "\n",
    "                val_outputs = model(val_inputs) # Forward pass\n",
    "                _, val_preds = torch.max(val_outputs, 1) # Get predicted class\n",
    "                val_loss = criterion(val_outputs, val_labels) # Compute validation loss\n",
    "\n",
    "                loss_current_val += val_loss.item() * val_inputs.size(0) # Accumulate validation loss\n",
    "                acc_current_val += torch.sum(val_preds == val_labels.data) # Count correct predictions\n",
    "\n",
    "        # Update best accuracy and hyperparameters\n",
    "        if acc_current_val > best_accuracy:\n",
    "            best_accuracy = acc_current_val / len(validation_dset)  # Compute validation accuracy\n",
    "            best_hyperparameters = {'learning_rate': lr, 'batch_size': batch_size, 'momentum': momentum}\n",
    "\n",
    "        ### STORE STATISTICS ###\n",
    "        epoch_loss = loss_current_train / len(training_dset)  # Compute training loss\n",
    "        summary_loss_train.append(epoch_loss)\n",
    "\n",
    "        val_epoch_loss = loss_current_val / len(validation_dset)  # Compute validation loss\n",
    "        summary_loss_val.append(val_epoch_loss)\n",
    "\n",
    "        epoch_acc = acc_current_train.float() / len(training_dset)  # Compute training accuracy\n",
    "        summary_acc_train.append(epoch_acc)\n",
    "\n",
    "        val_epoch_acc = acc_current_val.float() / len(validation_dset)  # Compute validation accuracy\n",
    "        summary_acc_val.append(val_epoch_acc)\n",
    "\n",
    "        # Save the best model if the accuracy improves\n",
    "        if val_epoch_acc > best_acc:\n",
    "            best_acc = val_epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Print training and validation performance per epoch\n",
    "        print('Epoca:', (e+1))\n",
    "        print('Training:   Loss {:.4f}, Acc {:.4f}'.format(epoch_loss, epoch_acc.item()))\n",
    "        print('Validation: Loss {:.4f}, Acc {:.4f}'.format(val_epoch_loss, val_epoch_acc.item()))\n",
    "        print(\"Best Hyperparameters:\", best_hyperparameters) # Stampa i migliori iperparametri e l'accuratezza corrispondente\n",
    "        print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "        # Create vector with 5 elements: 3 of the grid + training accuracy and validation accuracy.\n",
    "        epoca_to_vect = e+1\n",
    "        tr_acc_to_vect = '{:.2%}'.format(epoch_acc.item())\n",
    "        vl_acc_to_vect = '{:.2%}'.format(val_epoch_acc.item())\n",
    "        lr_to_vect = best_hyperparameters['learning_rate']\n",
    "        bs_to_vect = best_hyperparameters['batch_size']\n",
    "        mom_to_vect = best_hyperparameters['momentum']\n",
    "\n",
    "        vector_hyper = [epoca_to_vect, lr_to_vect, bs_to_vect, mom_to_vect, tr_acc_to_vect, vl_acc_to_vect]\n",
    "\n",
    "        # Store hyperparameter results every 10 epochs\n",
    "        if i_train % 10 == 0:\n",
    "            df_hyperparameters.loc[len(df_hyperparameters.index)] = vector_hyper\n",
    "\n",
    "            # Append results to CSV file\n",
    "            with open('hyperparameters_grid.csv', 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(vector_hyper)\n",
    "                csvfile.close()\n",
    "\n",
    "    # Save the best model after training completion\n",
    "    torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "\n",
    "# Compute total training time\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Y9gu6_XCWsm",
   "metadata": {
    "id": "0Y9gu6_XCWsm"
   },
   "source": [
    "# Attach Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60387494",
   "metadata": {
    "id": "60387494"
   },
   "outputs": [],
   "source": [
    "# Disable gradient computation for efficiency (no need to track gradients during validation)\n",
    "with torch.no_grad():\n",
    "    for val_inputs, val_labels in validation_loader: # Iterate over validation batches\n",
    "\n",
    "        # Move input data and labels to the correct device (GPU or CPU)\n",
    "        val_inputs = val_inputs.to(device)\n",
    "        val_labels = val_labels.to(device)\n",
    "\n",
    "        # Perform forward pass (get model predictions)\n",
    "        val_outputs = model(val_inputs)\n",
    "\n",
    "        # Get predicted class (highest probability)\n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "        # Accumulate total validation loss (scaled by batch size)\n",
    "        loss_current_val += val_loss.item() * val_inputs.size(0)\n",
    "\n",
    "        # Count correct predictions for accuracy calculation\n",
    "        acc_current_val += torch.sum(val_preds == val_labels.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ByAY_L2HB_WB",
   "metadata": {
    "id": "ByAY_L2HB_WB"
   },
   "source": [
    "## Random Forest hyperparameters optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9fa65",
   "metadata": {
    "id": "cab9fa65"
   },
   "outputs": [],
   "source": [
    "# List to store extracted CNN features and corresponding labels\n",
    "cnn_features_train = []\n",
    "labels_train = []\n",
    "\n",
    "# Function to register a hook on layer1 (feature extraction)\n",
    "def extract_layer1_features():\n",
    "    \"\"\"\n",
    "    Hook function to extract features from CUSTOM LAYER 1 during forward pass.\n",
    "    The extracted features are appended to the global list cnn_features_train.\n",
    "    \"\"\"\n",
    "    global cnn_features_train # Ensure features are stored globally\n",
    "    def hook(module, input, output):\n",
    "        cnn_features_train.extend(output.cpu().detach().numpy()) # Store extracted features\n",
    "    return hook\n",
    "\n",
    "# Register the hook on CUSTOM LAYER 1\n",
    "handle = model.fc.layer1.register_forward_hook(extract_layer1_features())\n",
    "\n",
    "# Set model to evaluation mode (no dropout, batch norm uses running stats)\n",
    "model.eval()\n",
    "\n",
    "# Extract features from the training set\n",
    "with torch.no_grad(): # Disable gradient computation for efficiency\n",
    "    for inputs, labels in training_loader: # Iterate over training dataset batches\n",
    "        inputs = inputs.to(device) # Move inputs to GPU/CPU\n",
    "        labels = labels.to(device) # Move labels to GPU/CPU\n",
    "\n",
    "        _ = model(inputs)# Forward pass (triggers hook function)\n",
    "\n",
    "        labels_train.extend(labels.cpu().numpy()) # Store corresponding labels\n",
    "\n",
    "# Remove the registered hook after feature extraction is complete\n",
    "handle.remove()\n",
    "\n",
    "# Convert extracted features and labels to numpy arrays for further processing\n",
    "X_train = np.array(cnn_features_train) # Feature matrix\n",
    "y_train = np.array(labels_train) # Label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f0be1",
   "metadata": {
    "id": "db7f0be1",
    "outputId": "341369dc-8919-4d30-bcd0-e45719923863"
   },
   "outputs": [],
   "source": [
    "# Assuming cnn_features_train and labels_train are extracted features and labels\n",
    "X = X_train # Feature matrix (CNN-extracted features)\n",
    "y = y_train # Label vector\n",
    "\n",
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "    'max_features': ['auto', 'sqrt']  # Number of features to consider for best split\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier with class balancing\n",
    "rf = RandomForestClassifier(class_weight='balanced') # Balances classes if they are imbalanced\n",
    "\n",
    "# Setup GridSearchCV with k-Fold cross-validation (k=5)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42) # Define cross-validation strategy\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring='accuracy') # Grid search with accuracy metric\n",
    "\n",
    "# Fit the model using grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Retrieve the best hyperparameters and the corresponding accuracy\n",
    "best_params = grid_search.best_params_ # Optimal hyperparameter combination\n",
    "best_score = grid_search.best_score_ # Best cross-validation accuracy\n",
    "\n",
    "# Print the best hyperparameters and accuracy\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best CV Accuracy: {:.2%}\".format(best_score)) # Print accuracy as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143e349",
   "metadata": {
    "id": "c143e349"
   },
   "outputs": [],
   "source": [
    "# Clear CUDA memory cache to free up unused GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define layers to be evaluated (ResNet backbone + custom classification head)\n",
    "layers_to_evaluate = {\n",
    "    'resnet_layer1': model_ft.layer1,  # First ResNet block\n",
    "    'resnet_layer2': model_ft.layer2,  # Second ResNet block\n",
    "    'resnet_layer3': model_ft.layer3,  # Third ResNet block\n",
    "    'resnet_layer4': model_ft.layer4,  # Fourth ResNet block\n",
    "    'resnet_avgpool': model_ft.avgpool,  # Global average pooling layer\n",
    "    'custom_linear1': model_ft.fc.get_layer('layer1'),  # First fully connected layer in custom head\n",
    "    'custom_bn1': model_ft.fc.get_layer('norm'),  # Batch normalization layer\n",
    "    'custom_relu': model_ft.fc.get_layer('relu'),  # ReLU activation function\n",
    "    'custom_dropout': model_ft.fc.get_layer('dropout'),  # Dropout layer\n",
    "    'custom_linear2': model_ft.fc.get_layer('layer2'),  # Final fully connected output layer\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid for Random Forest tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15],  # Maximum depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "    'max_features': ['auto', 'sqrt']  # Number of features considered for best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225ce57",
   "metadata": {
    "id": "3225ce57",
    "outputId": "ecb5cc7a-624f-4d9d-df75-33fdd96275d6"
   },
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01d955",
   "metadata": {
    "id": "bf01d955",
    "outputId": "a0afd300-ebc2-4c0c-a4bf-9c36fda42539"
   },
   "outputs": [],
   "source": [
    "# Idea for testing different layers for Random Forest feature extraction\n",
    "# Uses h5py for disk storage due to limited VRAM\n",
    "\n",
    "import h5py # Library for efficient disk-based data storage\n",
    "\n",
    "def extract_features(model, dataloader, layer, device):\n",
    "    \"\"\"\n",
    "    Extract features from a specific layer of the model using forward hooks.\n",
    "\n",
    "    Args:\n",
    "      model (torch.nn.Module): The neural network model.\n",
    "      dataloader (torch.utils.data.DataLoader): DataLoader for processing the dataset.\n",
    "      layer (torch.nn.Module): The specific layer to extract features from.\n",
    "      device (str): The device to run inference on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "      np.ndarray: Extracted features.\n",
    "      np.ndarray: Corresponding labels.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Hook function to capture layer outputs\n",
    "    def hook(module, input, output):\n",
    "        # Flatten the output to 2D (samples x features)\n",
    "        output_flat = output.view(output.size(0), -1) # Flatten output to 2D (samples x features)\n",
    "        features.append(output_flat.cpu().detach().numpy()) # Store features in CPU memory\n",
    "\n",
    "    handle = layer.register_forward_hook(hook) # Register hook on the layer\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation for efficiency\n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs = inputs.to(device) # Move inputs to the selected device\n",
    "            model(inputs) # Forward pass to trigger the hook\n",
    "            labels.extend(batch_labels.numpy()) # Store labels\n",
    "\n",
    "    handle.remove() # Remove the hook after feature extraction\n",
    "\n",
    "    return np.concatenate(features), np.array(labels) # Convert to NumPy arrays\n",
    "\n",
    "# Define path for saving extracted features\n",
    "save_dir = 'F:\\\\ArchAIDE_nn\\\\test'\n",
    "os.makedirs(save_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "# Extract and save features for each selected layer\n",
    "for layer_name, layer in layers_to_evaluate.items():\n",
    "    print(f\"Processing {layer_name}...\")\n",
    "\n",
    "    # Extract features from training and validation sets\n",
    "    train_features, train_labels = extract_features(model_ft, training_loader, layer, device)\n",
    "    val_features, val_labels = extract_features(model_ft, validation_loader, layer, device)\n",
    "\n",
    "    # Save features to disk using h5py to avoid VRAM overuse\n",
    "    train_features_path = os.path.join(save_dir, f'train_features_{layer_name}.h5')\n",
    "    val_features_path = os.path.join(save_dir, f'val_features_{layer_name}.h5')\n",
    "\n",
    "    with h5py.File(train_features_path, 'w') as h5file:\n",
    "        h5file.create_dataset('features', data=train_features) # Save training features\n",
    "        h5file.create_dataset('labels', data=train_labels) # Save training labels\n",
    "\n",
    "    with h5py.File(val_features_path, 'w') as h5file:\n",
    "        h5file.create_dataset('features', data=val_features) # Save validation features\n",
    "        h5file.create_dataset('labels', data=val_labels) # Save validation labels\n",
    "\n",
    "# Evaluate different layers with Random Forest\n",
    "best_accuracy = 0 # Track best accuracy\n",
    "best_layer = None # Store the best-performing layer\n",
    "\n",
    "for layer_name in layers_to_evaluate.keys():\n",
    "    # Load features from disk\n",
    "    with h5py.File(os.path.join(save_dir, f'train_features_{layer_name}.h5'), 'r') as h5file:\n",
    "        train_features = h5file['features'][:]\n",
    "        train_labels = h5file['labels'][:]\n",
    "\n",
    "    with h5py.File(os.path.join(save_dir, f'val_features_{layer_name}.h5'), 'r') as h5file:\n",
    "        val_features = h5file['features'][:]\n",
    "        val_labels = h5file['labels'][:]\n",
    "\n",
    "    # Initialize Random Forest classifier with predefined hyperparameters\n",
    "    rf_classifier = RandomForestClassifier(max_depth=15, n_estimators=200, min_samples_leaf=2, min_samples_split=2, criterion='gini', class_weight='balanced')\n",
    "\n",
    "    # Train the Random Forest classifier\n",
    "    rf_classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    val_predictions = rf_classifier.predict(val_features)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions) # Compute accuracy\n",
    "\n",
    "    print(f\"Layer {layer_name}: Validation Accuracy: {val_accuracy:.2%}\")\n",
    "\n",
    "    # Update best accuracy and best-performing layer\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_layer = layer_name\n",
    "\n",
    "# Print the best-performing layer for feature extraction\n",
    "print(f\"The best layer for feature extraction is: {best_layer} with an accuracy of {best_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b8837",
   "metadata": {
    "id": "cd3b8837"
   },
   "outputs": [],
   "source": [
    "# Idea for testing different layers for Random Forest feature extraction with Grid Search\n",
    "# Uses h5py for disk-based processing due to limited VRAM\n",
    "\n",
    "#takes way too long\n",
    "import h5py\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(model, dataloader, layer, device):\n",
    "    \"\"\"\n",
    "    Extract features from a specified layer of the model using forward hooks.\n",
    "\n",
    "    Args:\n",
    "      model (torch.nn.Module): The trained neural network model.\n",
    "      dataloader (torch.utils.data.DataLoader): DataLoader for processing the dataset.\n",
    "      layer (torch.nn.Module): The specific layer to extract features from.\n",
    "      device (str): The device to run inference on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "      np.ndarray: Extracted feature vectors.\n",
    "      np.ndarray: Corresponding labels.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Hook function to capture layer outputs\n",
    "    def hook(module, input, output):\n",
    "        # Flatten the output to 2D (samples x features)\n",
    "        output_flat = output.view(output.size(0), -1) # Flatten output to 2D (samples x features)\n",
    "        features.append(output_flat.cpu().detach().numpy()) # Store features on CPU memory\n",
    "\n",
    "    handle = layer.register_forward_hook(hook) # Register hook on the layer\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient computation for efficiency\n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs = inputs.to(device) # Move inputs to the selected device\n",
    "            model(inputs) # Forward pass triggers the hook\n",
    "            labels.extend(batch_labels.numpy()) # Store corresponding labels\n",
    "\n",
    "    handle.remove() # Remove the hook after feature extraction\n",
    "\n",
    "    return np.concatenate(features), np.array(labels) # Convert lists to NumPy arrays\n",
    "\n",
    "# Define path for saving extracted features\n",
    "save_dir = 'F:\\\\ArchAIDE_nn\\\\test'\n",
    "os.makedirs(save_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "# Track best-performing layer and parameters\n",
    "best_overall_accuracy = 0\n",
    "best_overall_layer = None\n",
    "best_overall_params = None\n",
    "\n",
    "# Loop through different layers and evaluate their extracted features\n",
    "for layer_name, layer in layers_to_evaluate.items():\n",
    "    print(f\"Processing {layer_name}...\")\n",
    "    # Extract features for training and validation\n",
    "    train_features, train_labels = extract_features(model_ft, training_loader, layer, device)\n",
    "    val_features, val_labels = extract_features(model_ft, validation_loader, layer, device)\n",
    "\n",
    "    # Define the hyperparameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],  # Number of trees\n",
    "        'max_depth': [5, 10, 15],  # Tree depth\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum samples to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "        'max_features': ['auto', 'sqrt']  # Number of features considered for best split\n",
    "    }\n",
    "\n",
    "    # Initialize Random Forest classifier\n",
    "    rf = RandomForestClassifier(class_weight='balanced') # Adjusts class imbalance automatically\n",
    "\n",
    "    # Setup GridSearchCV with k-Fold cross-validation (k=5)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42) # Define cross-validation strategy\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring='accuracy') # Grid search with accuracy metric\n",
    "\n",
    "    # Fit the model using Grid Search\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "\n",
    "    # Retrieve best hyperparameters and accuracy for this layer\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Layer {layer_name} - Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Layer {layer_name} - Best CV Accuracy: {best_score:.2%}\")\n",
    "\n",
    "    # Update best layer if accuracy is the highest so far\n",
    "    if best_score > best_overall_accuracy:\n",
    "        best_overall_accuracy = best_score\n",
    "        best_overall_layer = layer_name\n",
    "        best_overall_params = best_params\n",
    "\n",
    "# Print the best layer for feature extraction\n",
    "print(f\"The best layer for feature extraction is: {best_overall_layer} with an accuracy of {best_overall_accuracy:.2%} and parameters {best_overall_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609990a",
   "metadata": {
    "id": "a609990a"
   },
   "source": [
    "## CNN + Random Forest training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5dd8b7",
   "metadata": {
    "id": "1e5dd8b7"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Define the loss function\n",
    "epoch = 100 # Number of training epochs\n",
    "\n",
    "# Define hyperparameter grid\n",
    "batch_size = [16] # Batch size for training\n",
    "learning_rate = [0.001, 0.01] # Learning rate options\n",
    "momentum = [0.95] # Momentum for SGD optimizer\n",
    "\n",
    "# Initialize lists for tracking training and validation statistics\n",
    "summary_loss_train = []\n",
    "summary_acc_train = []\n",
    "summary_loss_val = []\n",
    "summary_acc_val = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict()) # Store best model weights\n",
    "best_acc = 0.0 # Store best validation accuracy\n",
    "\n",
    "since = time.time() # Track training start time\n",
    "\n",
    "i_train = 0\n",
    "\n",
    "# Create DataFrame to store hyperparameter results\n",
    "df_hyperparameters = pd.DataFrame(columns=['epoca', 'learning_rate', 'batch_size', 'momentum', 'training_acc', 'validation_acc'])\n",
    "\n",
    "# Create a CSV file for storing hyperparameter search results\n",
    "fields = ['epoca', 'learning_rate', 'batch_size', 'momentum', 'training_acc', 'validation_acc', 'training_acc_rf', 'validation_acc_rf']\n",
    "\n",
    "with open('hyperparameters_grid.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fields)\n",
    "    writer.writeheader()\n",
    "    csvfile.close()\n",
    "\n",
    "# Train model with different hyperparameter combinations\n",
    "for lr, batch_size, momentum in itertools.product(learning_rate, batch_size, momentum):\n",
    "\n",
    "    # Initialize optimizer with current hyperparameters\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size = 6, gamma = 0.1) # Learning rate decay every 6 epochs\n",
    "\n",
    "    # Reset model weights before training\n",
    "    model.apply(init_weights)\n",
    "    for e in range(epoch):\n",
    "\n",
    "        i_train += 1\n",
    "        ### TRAINING PHASE ###\n",
    "        model.train() # Set model to training mode\n",
    "        loss_current_train = 0.0\n",
    "        acc_current_train = 0.0\n",
    "\n",
    "        for inputs, labels in training_loader:\n",
    "            model = model.to(device) # Ensure model is on the correct device\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            loss = criterion(outputs, labels) # Compute loss\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step() # Update model weights\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) # Get predicted class\n",
    "            loss_current_train += loss.item() * inputs.size(0)\n",
    "            acc_current_train += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_acc = acc_current_train.float() / len(training_dset)\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        ### CNN VALIDATION PHASE ###\n",
    "        model.eval()\n",
    "        loss_current_val = 0.0\n",
    "        acc_current_val = 0.0\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient computation\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                loss_current_val += val_loss.item() * inputs.size(0)\n",
    "                acc_current_val += torch.sum(val_preds == val_labels.data)\n",
    "\n",
    "        # Track best validation accuracy\n",
    "        if acc_current_val > best_accuracy:\n",
    "            best_accuracy = acc_current_val / len(validation_dset)\n",
    "            best_hyperparameters = {'learning_rate': lr, 'batch_size': batch_size, 'momentum': momentum}\n",
    "\n",
    "        ### STORE TRAINING STATISTICS ###\n",
    "        epoch_loss = loss_current_train / len(training_dset)\n",
    "        summary_loss_train.append(epoch_loss)\n",
    "\n",
    "        epoch_acc = acc_current_train.float() / len(training_dset)\n",
    "        summary_acc_train.append(epoch_acc)\n",
    "\n",
    "        val_epoch_loss = loss_current_val / len(validation_dset)\n",
    "        summary_loss_val.append(val_epoch_loss)\n",
    "\n",
    "        val_epoch_acc = acc_current_val.float() / len(validation_dset)\n",
    "        summary_acc_val.append(val_epoch_acc)\n",
    "\n",
    "\n",
    "        # Store best model weights if accuracy improves\n",
    "        if val_epoch_acc > best_acc:\n",
    "            best_acc = val_epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('Epoca:', (e+1))\n",
    "        print('Training:   Loss {:.4f}, Acc {:.4f}'.format(epoch_loss, epoch_acc.item()))\n",
    "        print('Validation: Loss {:.4f}, Acc {:.4f}'.format(val_epoch_loss, val_epoch_acc.item()))\n",
    "        print(\"Best Hyperparameters:\", best_hyperparameters) # Stampa i migliori iperparametri e l'accuratezza corrispondente\n",
    "        print(\"Best Accuracy:\", best_accuracy) # c'è qualcosa che non va nella best accuracy\n",
    "\n",
    "        ### RANDOM FOREST FEATURE EXTRACTION ###\n",
    "        # Extract CNN features for training set\n",
    "        cnn_features_train = []\n",
    "        labels_train = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in training_loader:\n",
    "\n",
    "                model = model.to(device)\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                features_train = model(inputs).cpu().detach().numpy()\n",
    "                cnn_features_train.extend(features_train)\n",
    "                labels_train.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "        # Extract CNN features for validation set\n",
    "        cnn_features_val = []\n",
    "        labels_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "\n",
    "                model = model.to(device)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                features_val = model(inputs).cpu().detach().numpy()\n",
    "                cnn_features_val.extend(features_val)\n",
    "                labels_val.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        # Train a Random Forest classifier on extracted CNN features\n",
    "        rf_classifier = RandomForestClassifier(\n",
    "            max_depth=7, n_estimators=100, criterion='gini', class_weight='balanced'\n",
    "        )\n",
    "        rf_classifier.fit(cnn_features_train, labels_train)\n",
    "\n",
    "        # Evaluate Random Forest on training set\n",
    "        predictions_rf_train = rf_classifier.predict(cnn_features_train)\n",
    "        accuracy_rf_train = accuracy_score(labels_train, predictions_rf_train)\n",
    "        print(\"Random Forest train accuracy: {:.2%}\".format(accuracy_rf_train))\n",
    "\n",
    "        confusion_mat_train = metrics.confusion_matrix(labels_train, predictions_rf_train)\n",
    "        print(confusion_mat_train)\n",
    "\n",
    "        # Evaluate Random Forest on validation set\n",
    "        predictions_rf = rf_classifier.predict(cnn_features_val)\n",
    "        accuracy_rf = accuracy_score(labels_val, predictions_rf)\n",
    "        print(\"Random Forest accuracy: {:.2%}\".format(accuracy_rf))\n",
    "\n",
    "        confusion_mat = metrics.confusion_matrix(labels_val, predictions_rf)\n",
    "        print(confusion_mat)\n",
    "\n",
    "        ### STORE RESULTS ###\n",
    "        vector_hyper = [\n",
    "            e+1, best_hyperparameters['learning_rate'], best_hyperparameters['batch_size'],\n",
    "            best_hyperparameters['momentum'], \"{:.2%}\".format(epoch_acc.item()),\n",
    "            \"{:.2%}\".format(val_epoch_acc.item()), \"{:.2%}\".format(accuracy_rf_train),\n",
    "            \"{:.2%}\".format(accuracy_rf)\n",
    "        ]\n",
    "\n",
    "        # Append results to DataFrame every 10 epochs\n",
    "        if i_train % 10 == 0:\n",
    "            try:\n",
    "                new_row_df = pd.DataFrame([vector_hyper], columns=['epoca', 'learning_rate', 'batch_size', 'momentum', 'training_acc', 'validation_acc', 'training_acc_rf', 'validation_acc_rf'])\n",
    "                df_hyperparameters = pd.concat([df_hyperparameters, new_row_df], ignore_index=True)\n",
    "            except:\n",
    "                print(\"concat failed\")\n",
    "\n",
    "        # Append results to CSV file\n",
    "            with open('hyperparameters_grid.csv', 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(vector_hyper)\n",
    "                csvfile.close()\n",
    "\n",
    "    # Save best model weights after training completion\n",
    "    torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "\n",
    "# Compute total training time\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6cd4ebe272b099f59024bca10a61a1cf01fa0e0531180f45278d461e2897513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
